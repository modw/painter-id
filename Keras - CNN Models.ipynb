{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN - Block architecture\n",
    "In this notebook we develop models using the standard block structure of convolutional neural networks. The points reinforced are the same as the one seen in the reference below. We also compare our results to those of standard models such as LeNet-5. **The end goal is to have all of these models in a .py so that they can be imported from other notebooks.**\n",
    "\n",
    "Modifications should be around:\n",
    "- the number of conv layers (depending on input resolution)\n",
    "- the size of the conv kernels\n",
    "- whether or not to use dropout layers (probably)\n",
    "- which metrics to use (classification)\n",
    "- what parameters to use for the optimizer \n",
    "\n",
    "See https://github.com/riblidezso/peak_steepness/blob/master/neural_network_predictions/nn_utils.py and https://arxiv.org/pdf/1806.05995.pdf\n",
    "for references that use similarly \"abstract\" data although in a regression context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "# keras\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten,  AveragePooling2D, Conv2D\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# scikit learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "# data processing pipeline - see .py file\n",
    "from data_processing import processing_pipeline\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import convolve\n",
    "# for reloading\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining functions that return compiled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the network on top of the one seen at https://arxiv.org/pdf/1806.05995.pdf\n",
    "# their code: https://github.com/riblidezso/peak_steepness/blob/master/neural_network_predictions/nn_utils.py\n",
    "\n",
    "def get_ribli_model():  \n",
    "    model = Sequential()\n",
    "    model.name = 'ribli'\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(256, 256, 1), name='L01', activation='relu'))\n",
    "    model.add(Conv2D(32, (3, 3), name='L02', activation='relu'))\n",
    "\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2), name='L03'))\n",
    "    model.add(Conv2D(64, (3, 3), name='L04', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), name='L05', activation='relu'))\n",
    "    \n",
    "    model.add(AveragePooling2D(pool_size=(2, 2), name='L06'))\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), name='L07', activation='relu'))\n",
    "    model.add(Conv2D(128, (3, 3), name='L08', activation='relu'))\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2), name='L09'))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), name='L10', activation='relu'))\n",
    "    model.add(Conv2D(128, (3, 3), name='L11', activation='relu'))\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2), name='L12'))\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), name='L13', activation='relu'))\n",
    "    model.add(Conv2D(128, (3, 3), name='L14', activation='relu'))\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2), name='L15'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, name='L16', activation='relu'))\n",
    "    model.add(Dense(256, name='L17', activation='relu'))\n",
    "\n",
    "    model.add(Dense(2, name='L18'))\n",
    "    model.compile(optimizer=Adam(lr=1e-4, beta_1=0.9, beta_2=0.999), loss='mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pid_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.name = 'pid'\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=input_shape, name='L01', activation='relu'))\n",
    "    model.add(Conv2D(32, (3, 3), name='L02', activation='relu'))\n",
    "\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2), name='L03'))\n",
    "    model.add(Conv2D(64, (3, 3), name='L04', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), name='L05', activation='relu'))\n",
    "\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2), name='L06'))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), name='L07', activation='relu'))\n",
    "    model.add(Conv2D(128, (3, 3), name='L08', activation='relu'))\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2), name='L09'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, name='L10', activation='relu'))\n",
    "    model.add(Dense(256, name='L11', activation='relu'))\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet-5 as seen [here](https://www.pugetsystems.com/labs/hpc/The-Best-Way-to-Install-TensorFlow-with-GPU-Support-on-Windows-10-Without-Installing-CUDA-1187/#create-a-python-virtual-environment-for-tensorflow-using-conda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lenet5(input_shape):\n",
    "    # building LeNet without augmentation\n",
    "    model = Sequential()\n",
    "    model.name = 'lenet-5'\n",
    "    model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=input_shape) )\n",
    "    model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5193 patches.\n"
     ]
    }
   ],
   "source": [
    "# getting X and y - old data\n",
    "f1 = './data/sample_data/Farah_Pot_5cm_sq_15um.txt'\n",
    "f2 = './data/sample_data/Grant_Pot_5cm_sq_15um.txt'\n",
    "files = [f1,f2]\n",
    "\n",
    "patch_size = 64\n",
    "X, y = processing_pipeline(files, patch_size)\n",
    "print(f\"There are {len(y)} patches.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# for a quick test\n",
    "idx = np.random.choice(np.arange(len(y)),500)\n",
    "X, y = X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "# reshaping for Keras\n",
    "X_train = X_train.reshape(*X_train.shape,1)\n",
    "X_test = X_test.reshape(*X_test.shape,1)\n",
    "# getting y arrays for Keras\n",
    "n_classes = len(files)\n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Scaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
       "       with_scaling=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting some parameters\n",
    "input_shape = X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3894 samples, validate on 1299 samples\n",
      "Epoch 1/10\n",
      "3894/3894 [==============================] - 30s 8ms/step - loss: 0.6935 - acc: 0.5128 - val_loss: 0.6935 - val_acc: 0.5089\n",
      "Epoch 2/10\n",
      "3894/3894 [==============================] - 28s 7ms/step - loss: 0.6906 - acc: 0.5313 - val_loss: 0.6526 - val_acc: 0.6035\n",
      "Epoch 3/10\n",
      "3894/3894 [==============================] - 28s 7ms/step - loss: 0.6228 - acc: 0.6328 - val_loss: 0.6045 - val_acc: 0.6713\n",
      "Epoch 4/10\n",
      "3894/3894 [==============================] - 28s 7ms/step - loss: 0.5791 - acc: 0.6700 - val_loss: 0.5459 - val_acc: 0.7275\n",
      "Epoch 5/10\n",
      "3894/3894 [==============================] - 30s 8ms/step - loss: 0.5642 - acc: 0.6898 - val_loss: 0.5323 - val_acc: 0.7306\n",
      "Epoch 6/10\n",
      "3894/3894 [==============================] - 30s 8ms/step - loss: 0.5503 - acc: 0.7067 - val_loss: 0.5319 - val_acc: 0.7360\n",
      "Epoch 7/10\n",
      "3894/3894 [==============================] - 30s 8ms/step - loss: 0.5218 - acc: 0.7337 - val_loss: 0.5125 - val_acc: 0.7460\n",
      "Epoch 8/10\n",
      "3894/3894 [==============================] - 32s 8ms/step - loss: 0.5308 - acc: 0.7347 - val_loss: 0.5216 - val_acc: 0.7490\n",
      "Epoch 9/10\n",
      "3894/3894 [==============================] - 37s 9ms/step - loss: 0.5265 - acc: 0.7386 - val_loss: 0.5068 - val_acc: 0.7521\n",
      "Epoch 10/10\n",
      "3894/3894 [==============================] - 30s 8ms/step - loss: 0.4984 - acc: 0.7435 - val_loss: 0.5311 - val_acc: 0.7444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8b53c99cf8>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters for fit\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# fitting the model\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "          validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3894 samples, validate on 1299 samples\n",
      "Epoch 1/10\n",
      "3894/3894 [==============================] - 33s 9ms/step - loss: 0.7032 - acc: 0.6094 - val_loss: 0.6583 - val_acc: 0.6205\n",
      "Epoch 2/10\n",
      "3894/3894 [==============================] - 33s 8ms/step - loss: 0.5783 - acc: 0.6708 - val_loss: 0.6052 - val_acc: 0.6721\n",
      "Epoch 3/10\n",
      "3894/3894 [==============================] - 32s 8ms/step - loss: 0.5383 - acc: 0.7278 - val_loss: 0.6000 - val_acc: 0.6644\n",
      "Epoch 4/10\n",
      "3894/3894 [==============================] - 33s 8ms/step - loss: 0.4812 - acc: 0.7568 - val_loss: 0.6147 - val_acc: 0.6859\n",
      "Epoch 5/10\n",
      "3894/3894 [==============================] - 31s 8ms/step - loss: 0.4629 - acc: 0.7789 - val_loss: 0.6192 - val_acc: 0.6875\n",
      "Epoch 6/10\n",
      "3894/3894 [==============================] - 32s 8ms/step - loss: 0.3814 - acc: 0.8285 - val_loss: 0.6407 - val_acc: 0.7005\n",
      "Epoch 7/10\n",
      "3894/3894 [==============================] - 33s 8ms/step - loss: 0.3009 - acc: 0.8747 - val_loss: 0.7045 - val_acc: 0.7221\n",
      "Epoch 8/10\n",
      "3894/3894 [==============================] - 35s 9ms/step - loss: 0.2242 - acc: 0.9114 - val_loss: 0.8375 - val_acc: 0.6921\n",
      "Epoch 9/10\n",
      "3894/3894 [==============================] - 35s 9ms/step - loss: 0.1659 - acc: 0.9440 - val_loss: 0.8710 - val_acc: 0.7167\n",
      "Epoch 10/10\n",
      "3894/3894 [==============================] - 33s 8ms/step - loss: 0.1182 - acc: 0.9615 - val_loss: 0.9225 - val_acc: 0.7321\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8b53c99748>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenet = get_lenet5(input_shape)\n",
    "\n",
    "# fitting the model\n",
    "lenet.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "          validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet-5 is overfitting which we can clearly see by the acc and loss of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestamp = str(int(time.time()*1000000))[0:-1]\n",
    "# model.save(f'./models/{model.name}-{timestamp}.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    #featurewise_center = True,\n",
    "    #featurewise_std_normalization=True)\n",
    "    rotation_range=90,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "# fit it\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for fit\n",
    "batch_size = 32\n",
    "epochs = 15\n",
    "\n",
    "# fit model with data augmentation\n",
    "model.fit_generator(datagen.flow(X_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        steps_per_epoch= len(X_train)//batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        workers=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
